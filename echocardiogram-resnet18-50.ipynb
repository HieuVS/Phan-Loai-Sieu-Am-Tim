{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom torchvision import datasets, models, transforms\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torch.autograd import Variable\nimport time\nimport copy\n\ninput_path = \"../input/data-chamber/DATA_CHAMBER_2021/\" \nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:41.492285Z","iopub.execute_input":"2021-11-09T15:30:41.492888Z","iopub.status.idle":"2021-11-09T15:30:42.484001Z","shell.execute_reply.started":"2021-11-09T15:30:41.492836Z","shell.execute_reply":"2021-11-09T15:30:42.483258Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.torch'))\n\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n    \n!cp ../input/pretrained-pytorch-models/* ~/.torch/models/\n!ls ~/.torch/models","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:42.485262Z","iopub.execute_input":"2021-11-09T15:30:42.485528Z","iopub.status.idle":"2021-11-09T15:30:49.316908Z","shell.execute_reply.started":"2021-11-09T15:30:42.485482Z","shell.execute_reply":"2021-11-09T15:30:49.315990Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ImageFolderWithPaths(datasets.ImageFolder):\n    \"\"\"Custom dataset that includes image file paths. Extends\n    torchvision.datasets.ImageFolder\n    \"\"\"\n\n    # override the __getitem__ method. this is the method that dataloader calls\n    def __getitem__(self, index):\n        # this is what ImageFolder normally returns \n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        # the image file path\n        path = self.imgs[index][0]\n        # make a new tuple that includes original and the path\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:49.321488Z","iopub.execute_input":"2021-11-09T15:30:49.322013Z","iopub.status.idle":"2021-11-09T15:30:49.331805Z","shell.execute_reply.started":"2021-11-09T15:30:49.321961Z","shell.execute_reply":"2021-11-09T15:30:49.331019Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:49.333392Z","iopub.execute_input":"2021-11-09T15:30:49.335547Z","iopub.status.idle":"2021-11-09T15:30:49.342513Z","shell.execute_reply.started":"2021-11-09T15:30:49.335493Z","shell.execute_reply":"2021-11-09T15:30:49.341773Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract = False, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet18\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n        \n    elif model_name == \"resnet50\":\n        \"\"\" Resnet50\n        \"\"\"\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model('resnet18', 3, use_pretrained=True)\n\n# Print the model we just instantiated\n#print(model_ft)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:49.343690Z","iopub.execute_input":"2021-11-09T15:30:49.344256Z","iopub.status.idle":"2021-11-09T15:30:49.798015Z","shell.execute_reply.started":"2021-11-09T15:30:49.344210Z","shell.execute_reply":"2021-11-09T15:30:49.797236Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ndata_transforms = {\n    'train': transforms.Compose([   # Here we do not make data augmentations\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n         transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize\n    ]),\n    'validation': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        normalize\n    ]),\n}\n\n\n\nimage_datasets = {\n    'train': \n    ImageFolderWithPaths(input_path + 'train', data_transforms['train']),\n    'validation': \n    ImageFolderWithPaths(input_path + 'test', data_transforms['validation'])\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train','validation']}\n\ndataloaders = {\n    'train':\n    torch.utils.data.DataLoader(image_datasets['train'],\n                                batch_size=8,\n                                shuffle=True,\n                                num_workers=1),  # for Kaggle\n    'validation':\n    torch.utils.data.DataLoader(image_datasets['validation'],\n                                batch_size=8,\n                                shuffle=True,\n                                num_workers=1)  # for Kaggle\n}\nclass_names = image_datasets['train'].classes\n\n# See some statistics\nprint(dataloaders)\nlen(dataloaders['train'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:49.799244Z","iopub.execute_input":"2021-11-09T15:30:49.799495Z","iopub.status.idle":"2021-11-09T15:30:56.228635Z","shell.execute_reply.started":"2021-11-09T15:30:49.799453Z","shell.execute_reply":"2021-11-09T15:30:56.227859Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Visualize a few images","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nplt.ion()   # interactive mode\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n# def imshow(inp, title=None):\n#     inp = inp.numpy().transpose((1, 2, 0))\n#     # plt.figure(figsize=(10, 10))\n#     plt.axis('off')\n#     plt.imshow(inp)\n#     if title is not None:\n#         plt.title(title)\n#     plt.pause(0.001)\n\n\n# Get a batch of training data\ninputs, classes,_ = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out)\n\n# imshow(out, title=[class_names[x] for x in classes])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:56.233473Z","iopub.execute_input":"2021-11-09T15:30:56.236783Z","iopub.status.idle":"2021-11-09T15:30:57.071459Z","shell.execute_reply.started":"2021-11-09T15:30:56.236720Z","shell.execute_reply":"2021-11-09T15:30:57.070554Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 2. General functions to train and visualize\n\nHere we use a general function to train a model. It includes:\n\n* Scheduling the learning rate\n* Saving the best model\n\nWe use [*torch.optim.lr_scheduler*](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate). It provides several methods to adjust the learning rate based on the number of epochs. Our function parameter `scheduler` is an object from it.","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=2, is_inception=False):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    loss_values = []\n    acc_values = []\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train','validation']:\n            train_batches = len(dataloaders[phase])\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for i,(inputs, labels,_) in enumerate(dataloaders[phase]):\n                print(\"\\rTraining batch {}/{}\".format(i+1, train_batches), end='', flush=True)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    # mode we calculate the loss by summing the final output and the auxiliary output\n                    # but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                    \n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            loss_values.append(epoch_loss)\n            acc_values.append(epoch_acc)\n            print('\\n{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'validation' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model,acc_values,loss_values","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:57.076552Z","iopub.execute_input":"2021-11-09T15:30:57.077237Z","iopub.status.idle":"2021-11-09T15:30:57.096800Z","shell.execute_reply.started":"2021-11-09T15:30:57.077186Z","shell.execute_reply":"2021-11-09T15:30:57.095970Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the model predictions\n\nA generic function to display predictions for a few images.","metadata":{}},{"cell_type":"code","source":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels,_) in enumerate(dataloaders['validation']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:57.097784Z","iopub.execute_input":"2021-11-09T15:30:57.098232Z","iopub.status.idle":"2021-11-09T15:30:57.646754Z","shell.execute_reply.started":"2021-11-09T15:30:57.098178Z","shell.execute_reply":"2021-11-09T15:30:57.645778Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 3. Transfer learning: feature extractor\n\nHere we use **Inception v3** as a fixed feature extractor.\n\nHere, we need to freeze all the network except the final layer. We need to set `requires_grad == False` to freeze the parameters so that the gradients are not computed in `backward()`.\n\n### Inception v3\n\nInception v3 was first described in [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567v1.pdf). This network is unique because it has two output layers when training. \n\nThe second output is known as an auxiliary output and is contained in the AuxLogits part of the network. The primary output is a linear layer at the end of the network. \n\nNote, when testing we only consider the primary output. ","metadata":{}},{"cell_type":"markdown","source":"## 4. Train and evaluate\n\nWe use [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) to schedule the learning rate.","metadata":{}},{"cell_type":"code","source":"\ndef test_model(model, criterion, optimizer):\n    labels_input=list()\n    labels_output=list()\n    vid_id = list()\n    for phase in ['validation']:\n        model.eval()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels, fname in dataloaders[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            labels_input= labels_input + labels.tolist()\n            for f in fname:\n                vid_id.append(f.split('/')[-1].split('.')[0].split('_')[0])\n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            \n            labels_output= labels_output + preds.tolist()\n    return labels_input,labels_output,vid_id\n            \n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:57.648615Z","iopub.execute_input":"2021-11-09T15:30:57.649128Z","iopub.status.idle":"2021-11-09T15:30:57.659993Z","shell.execute_reply.started":"2021-11-09T15:30:57.648940Z","shell.execute_reply":"2021-11-09T15:30:57.659224Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_ft = model_ft.to(device)\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\nparams_to_update = []\nfor name,param in model_ft.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n\noptimizer_conv = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n\ncriterion = nn.CrossEntropyLoss()\n# Decay LR by a factor of 0.1 every epoch\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=1, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:30:57.661756Z","iopub.execute_input":"2021-11-09T15:30:57.661984Z","iopub.status.idle":"2021-11-09T15:31:01.508636Z","shell.execute_reply.started":"2021-11-09T15:30:57.661940Z","shell.execute_reply":"2021-11-09T15:31:01.507896Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model_ft,acc,loss = train_model(model_ft, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=10, is_inception=False) # As an example, only show the results of 2 epoch","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:31:01.509764Z","iopub.execute_input":"2021-11-09T15:31:01.510010Z","iopub.status.idle":"2021-11-09T15:45:43.795871Z","shell.execute_reply.started":"2021-11-09T15:31:01.509968Z","shell.execute_reply":"2021-11-09T15:45:43.794989Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# plot history","metadata":{}},{"cell_type":"code","source":"def plot_history(history,loss,acc): \n    fig, ax1 = plt.subplots()\n    \n    ax1.plot(loss, 'r', label=\"training loss\")\n    ax1.grid(True)\n    ax1.set_xlabel('iteration')\n    ax1.set_ylabel('loss', color='r')\n    ax1.legend(loc=\"best\", fontsize=9)    \n    ax1.tick_params('y', colors='r')\n\n    ax2 = ax1.twinx()\n    ax2.plot(acc, 'b', label=\"training acc\")\n    ax2.legend(loc=\"lower right\", fontsize=9)\n    ax2.set_ylabel('acc', color='b')        \n    ax2.tick_params('y', colors='b')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:45:43.797529Z","iopub.execute_input":"2021-11-09T15:45:43.797842Z","iopub.status.idle":"2021-11-09T15:45:43.806375Z","shell.execute_reply.started":"2021-11-09T15:45:43.797788Z","shell.execute_reply":"2021-11-09T15:45:43.804615Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plot_history(model_ft,acc,loss)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:45:43.807828Z","iopub.execute_input":"2021-11-09T15:45:43.808621Z","iopub.status.idle":"2021-11-09T15:45:44.255027Z","shell.execute_reply.started":"2021-11-09T15:45:43.808540Z","shell.execute_reply":"2021-11-09T15:45:44.254335Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\ny_true,y_pred,vid_id = test_model(model_ft, criterion, optimizer_conv)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:45:44.256258Z","iopub.execute_input":"2021-11-09T15:45:44.256691Z","iopub.status.idle":"2021-11-09T15:46:00.418236Z","shell.execute_reply.started":"2021-11-09T15:45:44.256643Z","shell.execute_reply":"2021-11-09T15:46:00.417288Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Acuracy score in test dataset\nFrame accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:46:00.421719Z","iopub.execute_input":"2021-11-09T15:46:00.422046Z","iopub.status.idle":"2021-11-09T15:46:01.353305Z","shell.execute_reply.started":"2021-11-09T15:46:00.421991Z","shell.execute_reply":"2021-11-09T15:46:01.352346Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Create a dataframe from ours image label, predictions and video_id of each image (frame)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport glob\n\n\ndf = pd.DataFrame(list(zip(y_true,y_pred,vid_id)),columns =['y_true','y_pred','vid_id'])\ndf.to_csv('df.csv',encoding='utf-8',index=False)\n\nvid_list = list(set(df['vid_id'].values))\ny_true = []\ny_pred = []\nfor vid in vid_list:\n    #print(vid)\n    tmp_df = df[df['vid_id']==vid]\n    #print(len(tmp_df))\n    vid_pred = tmp_df['y_pred'].mode().values[0]\n    vid_label = tmp_df['y_true'].mode().values[0]\n    y_true.append(vid_label)\n    y_pred.append(vid_pred)\n    #print(vid_label,\"\\n\",vid_pred)\n    \n    #print('vid: {} label: {} pred: {}'.format(vid,vid_label,vid_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:46:01.357991Z","iopub.execute_input":"2021-11-09T15:46:01.360425Z","iopub.status.idle":"2021-11-09T15:46:01.692878Z","shell.execute_reply.started":"2021-11-09T15:46:01.360358Z","shell.execute_reply":"2021-11-09T15:46:01.691918Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Calculate video acc base on voting scheme\n\nDominant predicted frame's labels will be voted to be video's labels","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:46:01.697680Z","iopub.execute_input":"2021-11-09T15:46:01.700014Z","iopub.status.idle":"2021-11-09T15:46:01.710360Z","shell.execute_reply.started":"2021-11-09T15:46:01.699958Z","shell.execute_reply":"2021-11-09T15:46:01.709508Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}